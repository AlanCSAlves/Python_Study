 IA e Machine Learning

Interligencia Artificial // Machine Learning // Deep Learning

Inteligencia Arttificial se trata do conceito de aplicar a máquinas tarefas que deveriam ser realizadas por humanos, afim de otimizar processos.

Machine Learning é o processo de aprendizado das máquinas, o que permite que elas possam tomar decisões e tirar conclusões com base no treinamento ou não.

Deep Learning é o aprendizado profundo e contínuo.

NLP - Processamento de Linguagem Natural (Interpretação de Texto)

É possível implementar em nuvem e também podemos usar bibliotecas específicas.

É possível aplicar para:
  * Análise de Sentimento;
  * Tradução (definir o contexto da tradução);
  * Classificação de Frases (interpretação das frases e classificação);
  * Extração de Entidades (nome que se dá para um termo em específico);
  * Entre Outras Diversas Funções.

Pré-processamento para aplicação da NLP

1 - Tokenização - Separação da frase em elementos específicos.
Exemplo: 
Analisando por palavra = "Eu sou o negro" // Aplicando a Tokenização = ArrayFrase = ['Eu','sou', 'o','negro'] - Cada palavra é um token.
Analisando por frase = Eu sou negro, porque nasci negro. Meus pais são negros. // Aplicando a Tokenização = ArrayTexto = ['Eu sou negro', 'porque nasci negro.' , 'Meus pais são negros.'] - Cada frase é um token.

2 - Stopwords - Remoção de Stopwords, palavras sem peso relevante nas nossas frases para que possamos manter somente elementos relevantes como verbos, nomes, adjetivos, etc.
Exemplo: 
Analisando a frase = "Eu quero comprar uma televisão de 52 polegadas para ver o BBB".
Removendo Stopwords = "Eu quero comprar televisão 52 polegadas ver BBB".

**Não é obrigatório utilizar em todos os modelos, depende muito do objetivo da análise.

3 - Steminização - Remover o genero, suficxo, plural, caracteres das palavras para que em alguns casos possamos padronizar e interpretar as palavras como um
Exemplo:
Temos 'colaboradores' e 'colaboradoras' em um mesmo contexto // com a Steminização ficaria 'colaborad'
Temos 'carros' e 'carro' em um mesmo contexto // com a Steminização ficaria 'carr'

**Não é obrigatório utilizar em todos os modelos, depende muito do objetivo da análise.

Esses 3 passos são os mais comuns em uma análise, mas podemos aplicar diversos passos de acordo com a necessidade como Lowercasing, Uppercasing, remoção de palavras específicas, etc.

Ao aplicar o pré-processamento em um determinado conjunto de texto, o NLP aplica a NLC - Classificação de Texto ou a NER - Extração de Entidade

NLC - Classificação de Texto 
É realizado o treinamento de um modelo de modo a definir um peso para cada uma das classes de frase, ou seja, o sistema pega um conjunto específico de informações para determinar que o peso alguns elementos correspondam a uma classe especifica.

Exemplo:

1º Classe = Frases com um peso entre 99.5 e 100
2º Classe = Frases com um peso entre 70 e 80
3º Classe = Frases com um peso entre 19 e 20

O treinamento de classificação é aplicado.

É feito um input de uma frase específica dentro do sistema.

O sistema irá realiza a depuração da frase e atribuir um valor para cada um de seus elementos, o que permite que a frase possua um peso de 85 por exemplo.

O sistema de classificação irá pegar essa frase e calcular a aproximação entre os valores das classes, de modo que a classificação seja definida pelo valor que mais se aproxima na escala.

1º Classe = Frases com um peso entre 99.5 e 100
2º Classe = Frases com um peso entre 70 e 80
3º Classe = Frases com um peso entre 19 e 20

A frase se enquadraria mais na 2º Classe, por possuir um valor mais aproximado dentro do modelo.

Quanto menor a distancia entre a frase a classe, maior é a confiança de que o sistema está classificando corretamente. 

NER - Extração da Entidade
É realizado o treinamento do modelo para identificar os principais termos relevantes (principais entidades) de um determinado contexto e assim atribuir uma classificação de acordo com o seu tipo

Exemplo:

Quero solicitar um CARTÃO de CRÉDITO // Sistema separa as entidades CARTÃO e CRÉDITO e classificam = CARTÃO como um Produto e CRÉDITO como Tipo Cartão 
Quero mais LIMITE no meu CARTÃO // Sistema separa as entidades LIMITE e CARTÃO e classificam = LIMITE como Valor Cartão e CARTÃO como um Produto 
Quero contratar CRÉDITO PESSOAL // Sistema separa as entidades CRÉDITO e PESSOAL e classificam = CRÉDITO como Produto e PESSOAL como Tipo Crédito 

Ao identificar essas entidades, o sistema irá encaminhar o input para um determinado conjunto de classes ja cadastrado.

Nos casos acima, ele irá encaminhar o usuário para uma resposta previamente cadastrada para cada um dos questionamentos, mediante a interpretação dos conjuntos.

Ele também pode aplicar o Fuzzy Matching para conseguir identificar alguns termos específicos que podem ter sido digitados de forma errada (tipo caltão e crérito por ex., o sistema pode aplicar o match e checar que as palavras são cartão e crédito).

O Fuzzy Matching pode funcionar bem em pequenas distorções.

Os dois modelos de classificação permite realizar a interpretação do input e assim realizar a análise necessária (atendimento específico no caso de serviços, tradução de um determinado termo, entre outros use cases)

Processamento de Audio

SST // Speech to Text (Speech Recognizer) - Reconhecimento de fala que reconhece o audio e transcreve para texto, desde que o modelo esteja codificado com o idioma de fala. Também é possível definir o idioma por meio da fala de input.
TTS // Text to Speech - Reconhecimento de texto de modo a encontrar e encaixar as sílabas fonéticas para a pronuncia de uma frase.
Voice Recognition - Autenticação por meio do input da voz, a IA capta e grava uma voz específica e reconhece os parâmetros para identificar uma pessoa. Reconhecimento mais pessoal e particular.

É possível aplicar o modelo para o reconhecimento de determinados sinais de modo que uma maquina possa interpretar um determinado som e assim definir uma ação.

Para o treinamento dos modelos, podemos utilizar serviços de Cloud ou construir o próprio modelo.

Machine Learning Supervisionado - Quando possuimos uma base para realizar o treinamento do modelo, possuimos uma massa de treino para realizar o input dos dados.

O sistema irá realizar os cálculos de acordo com os inputs e definir os outputs, de modo que irá realizar um comparativo entre o previsto e o real para o cálculo da função perda.

Para realizar este procedimento devemos pegar a massa de dados e separar:

Train Data Set x Test Data Set.

80% da massa = Train para criação e treinamento do modelo // 20% da massa = Teste para validar a eficiencia do modelo.

Modelos Supervisionados podem ser:
**Regressão = Para a previsão ou predição de um valor numérico contínuo. Ex:. Prever um valor de aluguel, valor de um carro, volume de vendas, etc.
***Regressão Multipla - Muitas características para os dados (muitas variáveis) // Regresão Simples - Somente uma variável.
Existem diversos tipos de Regressões
**Classificação = Com base em um conjunto geral de dados, o modelo permite rotular um item de acordo com os seus parametros.
Assim como os modelos de Regressão, os tipos de classificação podem ser diversos.

Dentro dos 2 exemplos, existem diversas meios de se realizar o processo.

Métricas de Avaliação dos modelos de Regressão:
R Square / Adjusted R Square - Mede o quanto o modelo consegue se ajustar às mudanças das variáveis de entrada.
O R Square não pode ficar muito abaixo de 1, pois isso demonstraria que o modelo não está muito ajustado aos dados (Underfitting)
O R Square não pode ficar próximo ou ser exatamente 1, pois isso demonstraria que o está extremamente preso ao conjunto de dados, o que poderia gerar distorções nos resultados (Overfitting)
Uma métrica que auxilia no ajuste do modelo seria o Adjusted R Square, pois ela penaliza o modelo quando ele se ajusta demais a massa de treino.

MSE/RMSE (Mean Square Error // Root Mean Square Error) - Mede a qualidade geral do modelo através da média de erros na massa de teste e eleva ao quadrado
MSE - Utiliza o quadrado da média de erros na massa de teste. O RMSE é a raíz quadrada do MSE, pois o MSE pode se tornar um número muito grande, sua raíz quadrada facilitaria a análise.

MAE (Mean Absolut Error) - Média absoluta dos erros, considerando que todos os erros são iguais.

Quando usar cada uma das metricas:
R Square / Adjusted R Square - Necessário quando precisamos apresentar o modelo a alguém, o fato dele girar em torno de 0 a 1 pode facilitar através do cálculo porcentual.
MSE/RMSE/MAE - Necessário para identificar qual seria o melhor modelo, qual a melhor curva de regressão para o nosso Use Case.

Podemos utilizar as metricas do MSE e RMSE caso seja necessário penalizar muito modelo por cometer erros. Se o uso não for muito rigoroso e os erros fazem algum sentido no Use Case geral, podemos utilizar o MAE.

Métricas de Avaliação dos modelos de Classificação:

Antes de distrinchar as métricas, precisar interpretar o conceito de Matriz de Confusão:

Verdadeiro Negativo // Verdadeiro Positivo = Quando os valores são classificados corretamente (Negativo como Negativo // Positivo como Positivo)
Falso Negativo // Falso Positivo = Quando os valores não são classificados corretamente (Negativo como Positivo // Positivo como Negativo)

A métricas são calculadas como base nas respostas do modelo:

Acuracidade = (Verdadeiro Negativo + Verdadeiro Positivo) / Amostra Total - Esta métrica avalia as respostas corretas, quanto o modelo classificou corretamente.
Precisão = Verdadeiro Positivo / (Verdadeiro Positivo + Falso Positivo) = Verdadeiro Positivo / Total Previsto como Positivo - Esta métrica é aplicada quando o risco de classificarmos um falso positivo é alto, essa métrica acaba penalizando o modelo.
Recall = Verdadeiro Positivo / (Verdadeiro Positivo + Falso Negativo) = Verdadeiro Positivo / Total Positivo Atual - Esta métrica é aplicada quando o risco de classificarmos um falso negativo é alto, essa métrica também penaliza o modelo.

F1 Score = 2 x (Precisão * Recall) / (Precisão + Recall) - Avalia o modelo utilizando tanto a Precisão quanto o Recall, de modo a penalizar o modelo quando a classificação é incorreta. O objetivo do F1 Score é manter um equilibrio entre os erros.

K-Fold Cross Validation - Essa validação permite que possamos dividir a massa teste em partes iguais para que em cada interação possamos variar as partes em massas de Treino/Teste

Ex:
Interação 1 - TESTE // Treino // Treino // Treino
Interação 2 - Treino // TESTE // Treino // Treino
Interação 3 - Treino // Treino // TESTE // Treino
Interação 4 - Treino // Treino // Treino // TESTE

A cada rodada devemos extrair os parametros do modelo para verificar a sua assertividade.

Underfitting - O modelo está muito fraco, de modo que não consegue prever ou predizer com efetividade novos inputs.
Overfitting - Oposto do Under, com o Over modelo está muito ajustado para a massa de teste, de modo que não consegue prever ou predizer com efetividade novos inputs, por não se adaptar.

O modelo não precisa ser exatamente perfeito, ele precisa ser robusto e maleável com a entrada de novos dados, de modo a não variar muito nas classificações, sendo mais generalista com os dados.

Machine Learning Não Supervisionado - Quando não possuímos uma base para realizar o treinamento do modelo, o modelo por si só deve encontrar padroes e similaridades para realizar as análises.

Através da detecção de Anomalias ele observa os aspectos e classifica as respostas.

Modelos Supervisionados podem ser:
Clusterização - Agrupamento dos dados em categorias semelhantes, como desterminados padrões semelhantes.
Podendo ser:
Clusterização Exclusiva (Exclusive Clustering) - Um agente não pode participar de mais de uma categoria ao mesmo tempo
Clusterização Sobreposta (Overlapping Clustering) - Um agente pode participar de mais de uma categoria
Clusterização Hierarquica - Quebrando as informações por hierarquia.

Pode ser aplicado de diversas formas.

Modelo de Recomendação - O algoritmo detecta os padrões de comportamento e sugerem algo.
Podendo ser:

Content Based-Filtering 
**O sistema, com base no seu User Rating x Matriz de Categoria de Produtos, define uma pontuação para o seu User Profile e calcula um padrão de consumo de acordo com as categorias dos produtos mediante a interação do usuário.
Ao utilizar somente o critério de interação, o modelo só irá recomendar coisas que interagimos, nunca nada novo.

Colaborative Filtering
**O sistema não usa somente a sua interação de usuário, utiliza a de diversos outros usuários que possuem hajam similaridades entre si.
Desta forma, o sistema pode pontuar mais as interações e sugerir itens que outros perfis similares pontuaram mais.
Como o calculo de similaridade pode ser complexo, pode ser mais dificil esse sistema possuir escalabilidade.

Uma alternativa interessante seria desenvolver um modelo hibrido para que os modelos possam se complementar entre si.

Métricas de Avaliação:

Varia muito de acordo com o caso, pois as analises feitas pelo sistema podem ser diversas.

Para a Clusterização:
Slihouette Score - Fórmula calculada utilizando:
A = O centro de um grupo e a distancia médio de todos os elementos deste grupo
B = O centro de um grupo e a distancia outro grupo

Para a Recomendação:
MRR (Mean Reciprocal Rank) - Calcula a recomendação com base no seu gosto principal
MAP (Mean Average Precision) - Calcula a recomendação média para uma lista, porém a classificação é binária (relevante e não relevante)

